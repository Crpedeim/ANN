{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PITdpKqIXCwZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.datasets import fetch_openml\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.exceptions import DataDimensionalityWarning\n",
        "\n",
        "\n",
        "\n",
        "def relu(z):\n",
        "    return np.maximum(0, z)\n",
        "\n",
        "def relu_derivative(z):\n",
        "    return (z > 0).astype(float)\n",
        "\n",
        "def softmax(z):\n",
        "    # Subtract max for numerical stability (prevents exploding exponentials)\n",
        "    exp_z = np.exp(z - np.max(z))\n",
        "    return exp_z / np.sum(exp_z, axis=0, keepdims=True)\n",
        "\n",
        "print(\"Loading MNIST data...\")\n",
        "mnist = fetch_openml('mnist_784', version=1, as_frame=False, parser='pandas')\n",
        "images, target = mnist['data'], mnist['target']\n",
        "target = target.astype(int)\n",
        "\n",
        "encoded_target = to_categorical(target,10)\n",
        "\n",
        "w_h = np.random.uniform(-0.5,0.5,(128,784)) ## increased the hidden neuron size to 128 from 20 to increase accuracy\n",
        "w_o = np.random.uniform(-0.5,0.5,(10,128))\n",
        "\n",
        "b_h = np.zeros((128,1))\n",
        "b_o = np.zeros((10,1))\n",
        "\n",
        "epochs = 5\n",
        "\n",
        "count_correct = 0;\n",
        "\n",
        "alpha = 0.01;\n",
        "\n",
        "images = images/255;\n",
        "\n",
        "print(f\"Starting training on {len(images)} images...\")\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  for image, label_one_hot in zip(images,encoded_target):\n",
        "\n",
        "\n",
        "    image_column_vector = np.reshape(image,(-1,1))\n",
        "    label_column_vector = np.reshape(label_one_hot,(-1,1))\n",
        "\n",
        "    h_pre = w_h@image_column_vector + b_h\n",
        "  ##  h = 1/(1+np.exp(-h_pre)); changing to relu\n",
        "    h = relu(h_pre)\n",
        "\n",
        "\n",
        "    o_pre = w_o@h + b_o;\n",
        "## o = 1/(1+np.exp(-o_pre));  changing to softmax\n",
        "    o = softmax(o_pre)\n",
        "\n",
        "    error = 1/len(o) * np.sum((o-label_column_vector)**2,axis=0)\n",
        "\n",
        "    if np.argmax(o) == np.argmax(label_column_vector):\n",
        "      count_correct += 1;\n",
        "\n",
        "    # delta_o = (o - label_column_vector) * ( o * (1 - o) ) changing to softmax plus cross enhtrpy loss derivative\n",
        "\n",
        "    delta_o = o - label_column_vector\n",
        "\n",
        "\n",
        "    delta_h = (w_o.T @ delta_o) * relu_derivative(h_pre) ## from chain rule delta_h will be delta of next layer matrix multiplied by weights of next layer multipled by derivative of the activation function\n",
        "\n",
        "\n",
        "    w_o = w_o - alpha *(delta_o @ h.T) ## h.t came from chain rule and delta @ input gives the shape of weight matrix for any layer\n",
        "    w_h  = w_h - alpha * ( delta_h @ image_column_vector.T) ## from chain rule too h is also a linear function( W * H + B) so by chain rule we multiply by derivative of fucntion wrt w to complete_calculation\n",
        "\n",
        "\n",
        "\n",
        "    b_o = b_o - delta_o\n",
        "    b_h = b_h - delta_h\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Testing the network\n",
        "\n",
        "count_correct = 0;\n",
        "\n",
        "for image, label_one_hot in zip(images,encoded_target):\n",
        "\n",
        "\n",
        "  image_column_vector = np.reshape(image,(-1,1))\n",
        "  label_column_vector = np.reshape(label_one_hot,(-1,1))\n",
        "\n",
        "  h_pre = w_h@image_column_vector + b_h\n",
        "  # h = 1/(1+np.exp(-h_pre)); changing to relu activation functions for better accuracy\n",
        "  h = relu(h_pre)\n",
        "\n",
        "  o_pre = w_o@h + b_o;\n",
        "  # o = 1/(1+np.exp(-o_pre));\n",
        "  o = softmax(o_pre)\n",
        "\n",
        "\n",
        "  if np.argmax(o) == np.argmax(label_column_vector):\n",
        "    count_correct += 1;\n",
        "\n",
        "\n",
        "    ## Printing accuracy\n",
        "\n",
        "print(\"Accuracy is : \")\n",
        "\n",
        "print(count_correct/len(images) * 100)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# NEURAL NETWORK MATH & GENERALIZATIONS\n",
        "# =============================================================================\n",
        "\n",
        "# --- 1. NOTATION ---\n",
        "# L       : Current layer index\n",
        "# A[L]    : Activation (output) of layer L. (Input is A[0])\n",
        "# Z[L]    : Pre-activation (linear input) of layer L -> Z[L] = W[L] @ A[L-1] + B[L]\n",
        "# W[L]    : Weight matrix connecting layer L-1 to L\n",
        "# m       : Batch size (number of images processed at once)\n",
        "\n",
        "# --- 2. FORWARD PROPAGATION ---\n",
        "# The general formula for any layer L:\n",
        "# Z[L] = W[L] @ A[L-1] + B[L]\n",
        "# A[L] = activation_function(Z[L])  <-- e.g., ReLU for hidden, Softmax for output\n",
        "\n",
        "# --- 3. BACKPROPAGATION (THE TWO GOLDEN RULES) ---\n",
        "\n",
        "# RULE A: Calculating the \"Error Term\" (Delta) for any Hidden Layer\n",
        "# We pull the error from the NEXT layer (L+1) backwards to the current layer (L).\n",
        "# Formula: Delta[L] = (W[L+1].T @ Delta[L+1]) * derivative_of_activation(Z[L])\n",
        "# Logic:   \"Distribute the future error back to where it came from,\n",
        "#           then switch off neurons that weren't active (derivative term).\"\n",
        "\n",
        "# RULE B: Calculating the Gradient for Weights (Delta_W)\n",
        "# To find how much to change weights, we combine the layer's error with its input.\n",
        "# Formula: Gradient_W[L] = (1/m) * (Delta[L] @ A[L-1].T)\n",
        "# Logic:   \"Error * Input\". If Input was high AND Error was high,\n",
        "#           this weight needs a big change.\n",
        "\n",
        "# --- 4. LOSS FUNCTIONS & SPECIAL CASES ---\n",
        "\n",
        "# Case 1: Output Layer with Softmax + Categorical Cross-Entropy\n",
        "# The complex derivatives cancel out perfectly to give a simple subtraction.\n",
        "# Delta_Output = A[Output] - Y_Target\n",
        "# (Where Y_Target is the one-hot encoded ground truth)\n",
        "\n",
        "\n",
        "# --- 5. WHY RELU & CROSS-ENTROPY WORK: VANISHING GRADIENTS ---\n",
        "#\n",
        "# Phenomenon: Switching from Sigmoid/MSE to ReLU/Cross-Entropy sped up learning.\n",
        "# Problem:    \"Vanishing Gradient Problem\". Sigmoid derivatives are small (<0.25).\n",
        "#             Multiplying small numbers in deep networks causes gradients to approach zero.\n",
        "# Solution:\n",
        "#   - ReLU: Derivative is either 0 or 1. Gradients flow through the network\n",
        "#     without shrinking, solving the vanishing gradient problem.\n",
        "#   - Cross-Entropy + Softmax: Creates a convex-like loss surface with steep gradients\n",
        "#     when predictions are wrong, forcing faster correction than MSE.\n",
        "\n",
        "\n",
        "# --- 6. WHY INCREASING SIZE WORKS: MODEL CAPACITY & UNIVERSAL APPROXIMATION ---\n",
        "#\n",
        "# Phenomenon: Increasing hidden neurons from 20 -> 128 improved accuracy (90% -> 96.7%).\n",
        "# Concept:    \"Model Capacity\" or \"Representational Power\".\n",
        "# Theorem:    The \"Universal Approximation Theorem\".\n",
        "#\n",
        "# Explanation:\n",
        "#   - A neural network with a single hidden layer can approximate ANY continuous\n",
        "#     function to arbitrary precision, *provided* it has enough neurons.\n",
        "#   - With 20 neurons, the network was \"Underfitting\". It lacked the memory/capacity\n",
        "#     to learn the complex shapes of all 10 digits simultaneously.\n",
        "#   - With 128 neurons, we increased the \"Hypothesis Space\", allowing the network\n",
        "#     to learn distinct features (loops, lines, curves) without interference.\n",
        "\n",
        "# Case 2: Standard Gradient Descent vs. Mini-Batch\n",
        "# If using Mini-Batch (m > 1):\n",
        "#   - Z and A become matrices of shape (Neurons, m)\n",
        "#   - We MUST divide the weight gradients by 'm' to keep updates stable.\n",
        "#   - Bias update: Gradient_B[L] = (1/m) * np.sum(Delta[L], axis=1, keepdims=True)\n",
        "\n",
        "# =============================================================================\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PR9xLKrbXrZB",
        "outputId": "12902103-87fb-4920-d62f-574578b9be2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading MNIST data...\n",
            "Starting training on 70000 images...\n",
            "Accuracy is : \n",
            "96.41714285714286\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cYJUX4JsYPal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4tTBYg6ybmoX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}